import{d as t,c as o,a,o as i}from"./index.17f7cf38.js";const r={class:"markdown-body"},l=a('<p>Some notes after crawling different data online using custom crawlers. This article will describe the crawling process where we find a way to locate the interesting resources and we archive them on a space efficient way. We could perform scraping on the go and extract the final data without saving the HTML to disk, but this is risky. An error on the scraper or some data piece missing from it will force you to download all the documents again and you don\u2019t this, believe me.</p><h2 id="exploration" tabindex="-1">Exploration</h2><ul><li>Find how to retrieve the data on the site (exploration): <ul><li>Find all the entries paginated and how to extract the links from the pagination pages.</li><li>Query the site search engine (either by using some word list or by brute forcing a dictionary or combinations with the alphabet -&gt; a, b \u2026 , aa, ab, \u2026)</li><li>Query some API of the site</li></ul></li></ul><p>Every method will have two identifiers at least: - Exploration identifier. This can be the page number if we are crawling some blog, the latest searched string if we are brute forcing the alphabet or the index of the latest word if we are brute forcing some word list. - Resource identifier. We want to download some data that usually will have some unique URL. This will usually be that URL.</p><h2 id="storage" tabindex="-1">Storage</h2><ul><li>Prepare some way to store the data: <ul><li>The best option so far is SQlite</li><li>MongoDB</li><li>Custom filesystem storage</li></ul></li></ul><h2 id="crawling-the-site" tabindex="-1">Crawling the site</h2><ul><li>Write some code to create an index of all the links: <ul><li>Download the first page -&gt; Download blog.com?page=1</li><li>Extract all the links with some scraper library or some regex</li><li>Save the links and the scraping status to disk. You may want save the links in memory and only save to disk every 10 pages or every 2000 links so the process runs a bit faster.</li><li>Iterate to next page</li></ul></li></ul><p>For traversing the pagination you need to design the code with getting back on the same spot where you left it. If you get blocked for doing too many request or your program crashes, you need a way to restore the process. So usually we will store the current page and the date on the database. Restoring the execution could be tricky on some cases, for example if you are crawling a blog, new entries could get publish between the last crawling session and now, therefore the saved page will be a bit off, in this case you will need to get back some pages and search for the latest downloaded entry.</p><h2 id="downloading-the-data" tabindex="-1">Downloading the data</h2><ul><li>Write some code to download the links: <ul><li>Do a GET request with your browser and extract the request headers. We want to copy and send each of them from our crawler. Lots of things could go wrong when messing this step. Watch out for headers related to: <ul><li>cache: When you don\u2019t use the proper cache header the website may return you old data, I once got some entries on an old layout that the website used years ago and found out when my failed on them.</li><li>encoding: If you don\u2019t use proper encoding, all the data downloaded could lose some special characters like <code>\xF1</code>, accents, \u2026</li><li>accept (html, json, \u2026).</li><li>user-agent: This one can mess every thing, some pages will control what html give you based on this fields. Strange, missing or invalid useragent can trigger anti-crawler code, etc.</li></ul></li><li>For every link, download its HTML. You can also save the response headers if you want, probably away from the HTML.</li><li>Mark the link as saved, use a HashSet in memory with all the downloaded links. If this grows way to much you may want to query the database. Anyway, store this registry of visited links from time to time on disk. Store a tuple like (url, visited_date). There are some ways of saving memory here like removing the domain from the url or hashing the url and storing the hash.</li><li>Compress the HTML with some algorithm, brotli works pretty good for HTML.</li><li>Store the downloaded html (now compress) with the date on tuples like (url, compress_html, date). Do this on batches.</li></ul></li></ul><h2 id="experiments-and-data-health-check" tabindex="-1">Experiments and data health check</h2><p>Before running the full program you will want to perform some experiment. The first one is measuring the maximum degree of parallelism. You can do a test to check how many documents you can download in parallel before getting blocked. Some website may let you hit it with up to 10 parallel request (todo, measure how many per minute). Depending on your needs you can stay a bet bellow the max or try to do a reasonable resource usage and crawl with a lower degree of parallelism.</p><p>The second test will be to run the process for some documents (ex download 10 entries). Check every detail of them, try to render them in your browser and try to fix its html, css, js by pointing to the resources on the website to make them work as a webpage. Do an effort trying to find if anything is odd, you want to be sure that every part of the process is perfect before spending 20 hours download entries and having to repeat the full process for some silly error.</p><ul><li>Todo: <ul><li>Expand SqlLite vs Mongo</li><li>Expand cache handling</li><li>Expand brotli</li><li>(todo, measure how many per minute)</li><li>Expand a bit SQLite features</li><li>Expand the data model</li><li>Article about crawler tooling</li><li>Article scraping</li></ul></li></ul>',15),s=[l],u={},m="",w=t({__name:"20220807_web_crawling",setup(n,{expose:e}){return e({frontmatter:{},excerpt:void 0}),(h,d)=>(i(),o("div",r,s))}});export{w as default,m as excerpt,u as frontmatter};
